## Neural Network Basics:
### Important definitions:
1. Structured data -> Is data that's rows of features with definite value
2. Unstructured data -> Data where features are left to be inferred. Like image / audio classification
3. More data --> Better performing algos --> RNNs. For a given data set of fixed size _m_, Perf: Logistic, etc < Small RNN < Medium RNN < Big RNN
4. Learning and correcting function can be sigmoid or ReLU. Sigmoid has a slower learning rate. ReLU is an on/off switch. So the learning rate is faster in ReLUs. Increased speed of computation with Relu.

### Logistic Regression as NN:
1. Convert unstructured to structured data, if possible. E.g. : Image to pixel values (RGB) 3 X 64 X 64 vector of features
2. _*IMPORTANT:*_ Unlike regular algos, in RNNs you have an input matrix of dimensions n by m instead of m by n. More on that later. 
3. Recall that wx + b = z = P(y = 1| x) is linear regression where if all x is negative, y might be negative. To  prevent the rubbish values, use Sigmoid(wx + b) = sigmoid(1/(1+ e^-(wx + b))). Higher the z => lower the e^z  => Higher value of probability. Closer to 1.
4. Also recall: differentiation gives error in range. So it is a valid LOSS function, using it to minimize MSE. But, if functions don't have a local optima or the curve changes convexity fast, it's not useful. So we use ylog p + (1-y)log(1-p) on BINARY classification. Use y = 0 and 1 to deduce why this function works.
5. Loss function = probability loss in one pair of input-output. Cost function of the parameters W,b =  RMSE(Loss function)/2
6. Differentiating gives steps to approach the point where slope is 0 or minima. w_n+1 = w_n + learning rate * slope at w_n. Curve is a function of W and b.So doe w, b /doe w and doe w/b / dow b approaches 0 at a global minima. Gradient ascent is w_n+1 = w_n - learnin rate * slope
7. Let J(a, b, c) = Cost function to be min = 3(a + bc).3, a, b, c ==>3, a, u(bc) ==>3, v (a + bc) ==> 3v (Ip)
8. Forward propogation ==> differentiate input vars. Backward propogation ==> Differentiate predicted output vars. In x--> y  x in terms of y is forward propogation and dy/dx is backward propogation. Essentially, we assume a function in terms of x and determine how erroneous it is via back prop. 
9. Week 2 last but one lecture, calculate dvars yourself. 
10. VECTORIZATION: Essential first step. np.dot(a, b) is a lot faster than elementwise a * b for a huge vector. Avoid for loops
